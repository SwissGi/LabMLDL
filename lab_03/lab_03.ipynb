{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGwNwDKEt8lG"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_03/hsg_logo.png?raw=1\">\n",
    "\n",
    "##  Lab 03 - \"Machine Learning with scikit-Learn\"\n",
    "\n",
    "Introduction to ML and DL, University of St. Gallen, Fall Term 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYpS4wEPt8lI"
   },
   "source": [
    "During the last two labs you learned the basics of Python programming. Today, we will start to use these skills to implement our first machine learning models. For this purpose, we will utilize the [`scikit-learn`](https://scikit-learn.org/stable/index.html) package, which provides a huge amount of functionality for different machine learning tasks, as well as some datasets for learning how to use this functionality. \n",
    "\n",
    "In this Notebook we will focus on implementing **supervised machine learning** methods, but be assured that utilizing **unsupervised** methods from `scikit-learn` is just as simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Br5f8mEt8lK"
   },
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0Jnx-Ljt8lK"
   },
   "source": [
    "## Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ybF-i5mQt8lL"
   },
   "source": [
    "Your learning objectives for today are threefold:\n",
    "\n",
    "> 1. Learn how to load, analyze and utilize **scikit-learn datasets**.\n",
    "> 2. Implement and train different **supervised classifier methods**.\n",
    "> 3. **Evaluate the results** of your trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZaa0qAnt8lY"
   },
   "source": [
    "## 0. Setup of the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yTCqemyt8la"
   },
   "source": [
    "Similarly to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the `Numpy`, `scikit-learn`, `Matplotlib` and the `Seaborn` library. Let's import the libraries by the execution of the statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "o3ShseCwt8lb",
    "outputId": "1254c7ff-5876-4508-8fde-5528e4d704f3"
   },
   "outputs": [],
   "source": [
    "# import the numpy, scipy and pandas data science library\n",
    "import numpy as np\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFnbcu4yt8le"
   },
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLbxWoZit8lf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsFqwDkYt8ln"
   },
   "source": [
    "Use the 'Seaborn' plotting style in all subsequent visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMH7Y9-Ht8lo"
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z2tRqzFt8lu"
   },
   "source": [
    "Set random seed of all our experiments - this insures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzE1FzaSt8lu"
   },
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0gpZzk5t8l5"
   },
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cilrWTyMt8l6"
   },
   "source": [
    "The **Iris Dataset** is a classic and straightforward dataset often used as a \"Hello World\" example in multi-class classification. This data set consists of measurements taken from three different types of iris flowers (referred to as **Classes**),  namely the Iris Setosa, the Iris Versicolour and the Iris Virginica, and their respective measured petal and sepal length (referred to as **Features**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlF-VYuOt8l7"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_03/iris_dataset.png?raw=1\">\n",
    "\n",
    "(Source: http://www.lac.inpe.br/~rafael.santos/Docs/R/CAP394/WholeStory-Iris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBHv_Rbrt8l8"
   },
   "source": [
    "In total, the dataset consists of **150 samples** (50 samples taken per class) as well as their corresponding **4 different measurements** taken for each sample. Please, find below the list of the individual measurements:\n",
    "\n",
    ">- `Sepal length (cm)`\n",
    ">- `Sepal width (cm)`\n",
    ">- `Petal length (cm)`\n",
    ">- `Petal width (cm)`\n",
    "\n",
    "Further details of the dataset can be obtained from the following puplication: *Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\"*\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Download and Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CtBrJGut8l9"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the dataset structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the dataset description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AE2PbwClt8mB"
   },
   "source": [
    "Print and inspect the names of the four features contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "NzLzNDo8t8mF",
    "outputId": "e336addc-0032-4f19-c65b-83a4482bc4a5"
   },
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIvnl8Qct8mK"
   },
   "source": [
    "Determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tq6gZN-1t8mM",
    "outputId": "8c985d93-12bb-4b17-e45d-6f284cedb17a"
   },
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwiIRMR_t8mW"
   },
   "source": [
    "Determine and print the class label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tayVqRQOt8mX",
    "outputId": "1ec43974-51bb-4117-e0e9-de84b82676bc"
   },
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RoQlbXs_t8md"
   },
   "source": [
    "Print and inspect the names of the three classes contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "R__ACqSct8me",
    "outputId": "f257226b-e22b-441c-db4a-50e47c9dad6c"
   },
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwqoNt8gt8mh"
   },
   "source": [
    "Let's briefly envision how the feature information of the dataset is collected and presented in the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCgJtdiot8mi"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"https://github.com/HSG-AIML/LabAIML/blob/main/lab_03/feature_collection.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rD3SBLxzt8mi"
   },
   "source": [
    "Let's inspect the top five feature rows of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "kju1z4Cft8mk",
    "outputId": "cf9f8028-e60b-4acf-dfd1-c6b2aaed1ddd"
   },
   "outputs": [],
   "source": [
    "iris.data[:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P62AsvZ8t8mr"
   },
   "source": [
    "Let's also inspect the top five class labels of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "oNjr0a5Dt8ms",
    "outputId": "160bca1e-1408-4904-efec-04a3a8939d97"
   },
   "outputs": [],
   "source": [
    "iris.target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fxz--vVdt8mu"
   },
   "source": [
    "Let's now conduct a more in depth data assessment. Therefore, we plot the feature distributions of the Iris dataset according to their respective class memberships as well as the features pairwise relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWofkTgQt8mw"
   },
   "source": [
    "Pls. note that we use Python's **Seaborn** library to create such a plot referred to as **Pairplot**. The Seaborn library is a powerful data visualization library based on the Matplotlib. It provides a great interface for drawing informative statstical graphics (https://seaborn.pydata.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "JmfO2-yit8mx",
    "outputId": "6a2392f8-a12e-4360-a5a8-acdf6bc9970d"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ugPoMiQt8m4"
   },
   "source": [
    "It can be observed from the created Pairplot, that most of the feature measurements that correspond to flower class \"setosa\" exhibit a nice **linear separability** from the feature measurements of the remaining flower classes. In addition, the flower classes \"versicolor\" and \"virginica\" exhibit a commingled and **non-linear separability** across all the measured feature distributions of the Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTWFzhhFt8m4"
   },
   "source": [
    "## 1.3 Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.1 Data Normalization\n",
    "\n",
    "As you can see in the plot above, the diffferent features span different ranges: *sepal_length* ranges from 4 to 8, *sepal_width* from 2 to 4 and so on. Those different ranges might affect the learning process negatively.\n",
    "\n",
    "In the case of $k$-Nearest Neighbor classification as we implement it below, the nearest neighbors are identified based on a distance measure. If the features are differently scaled, they are not considered equally important by the model. Consider the following example: the max-min distance in *sepal_length* is 4, whereas the same for *sepal_width* is only 2. When calculating the Euclidean distance between two datapoints, the contribution to the distance from *sepal_length* is likely to be larger than that from *sepal_width*. As a result, *sepal_length* has much more power and turns out to be more decisive in the resulting distance measure and classification results.\n",
    "\n",
    "To avoid this problem, one typically scales or normalizes the data in a uniform way. One way would be to scale the values in each feature relative to their maximum and minimum values, called the **min-max-scaler**:\n",
    "$$\\mathbf{x'} = \\frac{\\mathbf{x}-min(\\mathbf{x})}{max(\\mathbf{x})-min(\\mathbf{x})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the minimum and maximum values might be affected by outliers, a safer way would be to scale the data and the mean and standard deviation of the sample, which is sometimes called a **standard scaler**:\n",
    "$$\\mathbf{x'} = \\frac{\\mathbf{x}-mean(\\mathbf{x})}{\\sigma(\\mathbf{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard scaler forces upon the values in each feature a mean of unity and a spread that is based on the variance in the dataset. \n",
    "\n",
    "\n",
    "Let's use the standard scaler implemented in scikit-learn to scale our data. We have to import the correponding class and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the scaler and retrieve a transformed dataset, we can use the `.fit_transform()` method (the name might sound weird here, but will make sense later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = scaler.fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original data, mean =', np.mean(iris.data, axis=0))\n",
    "print('original data, std =', np.std(iris.data, axis=0))\n",
    "print('scaled data, mean =', np.mean(data_scaled, axis=0))\n",
    "print('scaled data, std =', np.std(data_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the scaled data in our machine learning model. \n",
    "\n",
    "Hint: If you would like to undo the scaling tranformation, you can use the `.inverse_transform()` method of `scaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original data:', iris.data[0,])\n",
    "print('scaled data:', data_scaled[0,])\n",
    "print('unscaled data:', scaler.inverse_transform(data_scaled[0,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTBwny8Dt8m5"
   },
   "source": [
    "### 1.3.2 Train, Validation and Test Datasets\n",
    "\n",
    "To understand and evaluate the performance of any trained **supervised machine learning** model, it is good practice to divide the dataset into a **training dataset** (the fraction of data records solely used for training purposes), a **validation dataset** (data to evaluate the current settings of your hyperparameters) and a **test dataset** (the fraction of data records solely used for independent evaluation purposes). Please note that both the **validation dataset** and the **test dataset** will never be shown to the model as part of the training process. The **test dataset** is sometimes also referred to as **evaluation set**; both terms refer to the same concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YN25KKcvt8m6"
   },
   "source": [
    "We first split our scaled dataset into a training dataset and some other dataset (which we will refer to as *remainder* in the following) that will then be evenly split into a validation and test dataset. We set the fraction of training records to **60%** of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPFvlzS6t8m7"
   },
   "outputs": [],
   "source": [
    "train_fraction = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FkQME8Ut8m9"
   },
   "source": [
    "Randomly split the scaled dataset into training set and evaluation set using sklearn's `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xF7m6KMSt8m9"
   },
   "outputs": [],
   "source": [
    "# 60% training and 40% remainder\n",
    "x_train, x_remainder, y_train, y_remainder = train_test_split(data_scaled, iris.target, test_size=1-train_fraction, \n",
    "                                                              random_state=random_seed, stratify=iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% validation and 50% test\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_remainder, y_remainder, test_size=0.5, \n",
    "                                                random_state=random_seed, stratify=y_remainder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T37IuZHIt8m_"
   },
   "source": [
    "Note the use of the `stratify` keyword argument here: a stratified split makes sure that approximately the same fraction of samples from each class is present in each dataset. Therefore, we have to provide the same list of class labels to this argument.\n",
    "\n",
    "Evaluate the different dataset dimensionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "N9i0U2uzt8nA",
    "outputId": "65cba01c-5c0e-4e75-e66d-e92cbdff8e29"
   },
   "outputs": [],
   "source": [
    "print('original:', iris.data.shape, iris.target.shape)\n",
    "print('train:', x_train.shape, y_train.shape)\n",
    "print('val:', x_val.shape, y_val.shape)\n",
    "print('test:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9HtRmw-t8nJ"
   },
   "source": [
    "# 2. Machine Learning\n",
    "\n",
    "Our goal for the Iris dataset is to be able to \"predict\" for each point in the dataset its class. This process is typically referred to as **classification** and is a standard task in Machine Learning (ML). \n",
    "\n",
    "As we already mentioned, we will utilize the `scikit-learn` package, which contains implementations of many different ML methods and tasks, both of supervised and unsupervised nature. \n",
    "\n",
    "Using `scikit-learn` is simple and its API is consistent throughout many different tasks and applications. We therefore discuss how to setup a machine learning pipeline with `scikit-learn` based on a simple $k$-nearest neigbhor classifier. Later we show that implementing other models is just as simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ns_yibVst8nK"
   },
   "source": [
    "## 2.1 $k$-Nearest Neighbor Classification\n",
    "\n",
    "The idea is simple: we assign to each unseen datapoint the same class that its $k$ nearest neighbors have. This method is intuitive and straightforward. It requires the definition of hyperparameter $k$ and the definition of a distance metric. Let's have a look at he `scikit-learn` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the method, we have to instantiate it by assigning the appropriate hyperparameters. We choose $k=3$ as our initial guess (and use the default `p=2`, utilizing the Euclidean distance metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model Training \n",
    "\n",
    "Now we have to train the model on our training dataset. Each model implemented in `scikit-learn` has a `.fit()` method for this purpose. \"Fitting\" refers here to the same idea that we typically refer to as \"training\", so don't get confused.\n",
    "\n",
    "The training of the data requires two `arrays`: the training input features ($\\mathbf{X}$) and the training target vector ($\\mathbf{y}$), such that for a given classifier $f$ the following holds: \n",
    "$$f(\\mathbf{X}) = \\mathbf{y}$$\n",
    "\n",
    "The way we split our dataset and into `x_train` and `y_train` already follows this naming convention. We can use those `arrays` readily in the training. Just for reference: `x_train` has to be of shape `(n_samples, n_features)` and `y_train` has to be of shape `(n_samples,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model` is now trained and can be used to make predictions. Let's take one datapoint from our training dataset and see whether it makes a correct prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it classifies this single datapoint correctly. However, this is not a good way to test or evaluate the performance of your model. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Model Evaluation\n",
    "\n",
    "Of course, we should use our previously split test sample for evaluating our model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick by-eye check seems to look pretty promising, but of course we need a more quantitative metric for the performance of our model.\n",
    "\n",
    "In the case of classification, we can use the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it. After evaluation on our independent eval dataset - which the model has not seen during training - we find that our model makes an accurate prediction in 93.3% of cases.\n",
    "\n",
    "This could be it, but there is a good chance that by tuning our sole hyperparameter, $k$, we can achieve a better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Hyperparameter Tuning\n",
    "\n",
    "Let's compile all the relevant code in one cell and try a different value for $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use a loop over different choices for $k$ and evaluate the model for these parameters to find the best-performing one. This process is called **hyperparameter tuning**.\n",
    "\n",
    "However, there is one more technical detail. Currently, we evaluate the performance on our **test dataset**. If we select $k$ based on these evaluations and therefore the **test dataset**, we have a *data leakage*. To resolve that issue, we can evaluate our model on the **validation dataset** for different $k$s and then, after picking the best-performing $k$, we can evaluate that model on the **test dataset**, providing an independent measure of performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 3, 5, 7, 10, 15, 20]:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_val)\n",
    "    print('k={:d}, val accuracy={:.2f}%'.format(k, accuracy_score(y_pred, y_val)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model performs equally well for $k\\sim3$ and $k\\sim10$. Based on experience, I would pick $k=10$. Why? For small values of $k$, you are more likely to overfit the training data, so choosing a larger value of $k$ increases the chances that the model generalizes well to data it has never seen before.\n",
    "\n",
    "Let's retrain the model with $k=10$ and we're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print('final test accuracy={:.2f}%'.format(accuracy_score(y_pred, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, evaluating the model on the test dataset provides the same accuracy as for the validation dataset - but this is not always the case, since both datsets are different from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Confusion Matrix\n",
    "\n",
    "So far, we have only considered the accuracy metric to evaluate our predictions. It would be useful to know whether one class of flower is more likely to be mistaken than another. For that purpose, confusion matrizes are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the y-axis you have your predicted classes, on the x-axis you have the actual classes. Those entries on the diagonal have been accurately predicted. Entries off the diagonal indicate how many flowers have incorrect class predictions.\n",
    "\n",
    "The ``seaborn`` library provides a method to generate nicely formatted confusion matrizes. Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Reds', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "\n",
    "# add plot title\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 More Models\n",
    "\n",
    "We found that a $k$-Nearest Neighbor classifier performs rather well on the Iris dataset. But what if we would like to try a different ML method?\n",
    "\n",
    "With `scikit-learn`, this is very easy to do. Since all algorithms (ML methods, scalers, etc.) are implemented as classes and thus provide `.fit()` and `.transform()` methods, we can simply replace them!\n",
    "\n",
    "Consider the following example. We use the exact same code that we use in tuning the $k$NN hyperparameters, but we replace the model with a **linear support vector machine** classifier implementation. Here, we replace the single hyperparameter $K$ with the regularization parameter $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "for C in [1e0, 1e1, 1e2, 1e3, 1e4]:\n",
    "    model = LinearSVC(C=C)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_val)\n",
    "    print('C={}, val accuracy={:.2f}%'.format(C, accuracy_score(y_pred, y_val)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's choose a **random forest** classifier. Here, we use two different hyperparameters: the number of trees in the ensemble and the maximum depth of the individual trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for n in [10, 50, 100, 200, 500]:\n",
    "    for d in [3, 7, 12]:\n",
    "        model = RandomForestClassifier(n_estimators=n, max_depth=d)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_val)\n",
    "        print('n={:3d}, d={:2d}, val accuracy={:.2f}%'.format(n, d, accuracy_score(y_pred, y_val)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very comparable, but the point is simply that with `scikit-learn`, you can pick any of the implemented methods and easily implement and try them in your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eGwNwDKEt8lG",
    "D0Jnx-Ljt8lK",
    "CZaa0qAnt8lY",
    "mMSfpCPvt8l4",
    "n94u0rxat8su"
   ],
   "name": "aiml_NB.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
